{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001-predictions-using-cloveai-polish.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQO5LkaozGTg"
      },
      "source": [
        "#  Package preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OreYCVZ-oBny"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCJwU9s3mRhs",
        "outputId": "3a855cab-921e-4597-f327-65009d956e6c"
      },
      "source": [
        "!pip3 install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yexptVNzMfe"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRhQwaYuoEHy",
        "outputId": "1068e777-58d6-4b4d-f811-cc1a70b1e477"
      },
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys\n",
        "from os.path import exists, join, basename, splitext\n",
        "import glob\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from ast import literal_eval as make_tuple\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed \n",
        "import codecs\n",
        "import argparse\n",
        "import unicodedata\n",
        "import editdistance\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K22dUe4nofS",
        "outputId": "c42bff27-e5e3-4c8a-fbf5-48dd8c9dab1a"
      },
      "source": [
        "%cd /content/\n",
        "git_repo_url = 'https://github.com/jandziak/deep-text-recognition-benchmark.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "project_name = project_name + '-pol'\n",
        "if not exists(project_name):\n",
        "  !git clone -q {git_repo_url} deep-text-recognition-benchmark-pol\n",
        "  !cd {project_name}\n",
        "  \n",
        "\n",
        "sys.path.append(project_name)\n",
        "from utils import CTCLabelConverter, CTCLabelConverterForBaiduWarpctc, AttnLabelConverter, Averager\n",
        "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset,RawDataset\n",
        "from model import Model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVxcd-Pnor35",
        "outputId": "9fec2465-a664-4b71-b238-36eefb07a241"
      },
      "source": [
        "%cd /content/\n",
        "git_repo_url = 'https://github.com/githubharald/CTCWordBeamSearch.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "\n",
        "if not exists(project_name):\n",
        "  !git clone -q {git_repo_url}\n",
        "  %cd {project_name}\n",
        "  !pip install .\n",
        "\n",
        "from word_beam_search import WordBeamSearch\n",
        "\n",
        "def wbs_decode(feedMat, corpus, chars, wordChars):\n",
        "    \"decode using word beam search. Result is tuple, first entry is label string, second entry is char string.\"\n",
        "\n",
        "    # decode using the \"Words\" mode of word beam search with beam width set to 25 and add-k smoothing to 0.0\n",
        "    assert len(chars) + 1 == feedMat.shape[2]\n",
        "    wbs = WordBeamSearch(25, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'), wordChars.encode('utf8'))\n",
        "    res = wbs.compute(feedMat)\n",
        "\n",
        "    # result is string of labels terminated by blank (similar to C-strings) if shorter than T\n",
        "    blank = len(chars)\n",
        "    s = ''\n",
        "\n",
        "    for label in res[0]:\n",
        "        if label == blank:\n",
        "            break\n",
        "        s += chars[label]  # map label to char\n",
        "    return res[0], s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/CTCWordBeamSearch\n",
            "Processing /content/CTCWordBeamSearch\n",
            "Building wheels for collected packages: word-beam-search\n",
            "  Building wheel for word-beam-search (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word-beam-search: filename=word_beam_search-1.0.0-cp36-cp36m-linux_x86_64.whl size=1177491 sha256=3543eec9800c3f1fa694b326e9dc9aa8f3faff6388272dda743a9546a29238b5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q8z03_bj/wheels/a9/69/4c/9d6acbecc7bf4b47c5072b213d9b08e4b9c43864bbed5206cc\n",
            "Successfully built word-beam-search\n",
            "Installing collected packages: word-beam-search\n",
            "Successfully installed word-beam-search-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmvnyjl0oQdg"
      },
      "source": [
        "def cut_top_part_image(image_name, input_folder, output_folder, cut_top): \n",
        "    in_path = input_folder + image_name\n",
        "    out_path = output_folder + image_name\n",
        "    image = cv2.imread(in_path)\n",
        "    cv2.imwrite(out_path, image[:cut_top,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfqqtwJkoRJA"
      },
      "source": [
        "def create_full_corpus(words):\n",
        "  return ' '.join(words.word.values)\n",
        "\n",
        "def create_corpus(filename, words, link_dict): \n",
        "  filename = filename.replace('__', '_')\n",
        "  filename = '_'.join(filename.split('_')[:-1])\n",
        "  res = link_dict[filename]\n",
        "  res = [i for i, v in enumerate(list(words.word.values)) if v == res[0] or v == res[1]]\n",
        "  if len(res) == 1:\n",
        "    corpus = words.word.values[res[0]]\n",
        "  else: \n",
        "    corpus = ' '.join(words.word.values[res[0]:res[1]+2])\n",
        "  return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySbavzCfoT87"
      },
      "source": [
        "def warpBox(image,\n",
        "            box,\n",
        "            target_height=None,\n",
        "            target_width=None,\n",
        "            margin=0,\n",
        "            cval=None,\n",
        "            return_transform=False,\n",
        "            skip_rotate=False):\n",
        "    \"\"\"Warp a boxed region in an image given by a set of four points into\n",
        "    a rectangle with a specified width and height. Useful for taking crops\n",
        "    of distorted or rotated text.\n",
        "    Args:\n",
        "        image: The image from which to take the box\n",
        "        box: A list of four points starting in the top left\n",
        "            corner and moving clockwise.\n",
        "        target_height: The height of the output rectangle\n",
        "        target_width: The width of the output rectangle\n",
        "        return_transform: Whether to return the transformation\n",
        "            matrix with the image.\n",
        "    \"\"\"\n",
        "    if cval is None:\n",
        "        cval = (0, 0, 0) if len(image.shape) == 3 else 0\n",
        "    if not skip_rotate:\n",
        "        box, _ = keras_ocr.tools.get_rotated_box(box)\n",
        "    w, h = keras_ocr.tools.get_rotated_width_height(box)\n",
        "    margin = h*margin\n",
        "    assert (\n",
        "        (target_width is None and target_height is None)\n",
        "        or (target_width is not None and target_height is not None)), \\\n",
        "            'Either both or neither of target width and height must be provided.'\n",
        "    if target_width is None and target_height is None:\n",
        "        target_width = w\n",
        "        target_height = h\n",
        "    scale = min(target_width / w, target_height / h)\n",
        "    M = cv2.getPerspectiveTransform(src=box,\n",
        "                                    dst=np.array([[margin, margin], [scale * w - margin, margin],\n",
        "                                                  [scale * w - margin, scale * h - margin],\n",
        "                                                  [margin, scale * h - margin]]).astype('float32'))\n",
        "    crop = cv2.warpPerspective(image, M, dsize=(int(scale * w), int(scale * h)))\n",
        "    target_shape = (target_height, target_width, 3) if len(image.shape) == 3 else (target_height,\n",
        "                                                                                   target_width)\n",
        "    full = (np.zeros(target_shape) + cval).astype('uint8')\n",
        "    full[:crop.shape[0], :crop.shape[1]] = crop\n",
        "    if return_transform:\n",
        "        return full, M\n",
        "    return full\n",
        "def calc_box(img_path, detector):\n",
        "    low_text_options = [0.3, 0.2, 0.1, 0.05, 0.01, 0.001, 0]\n",
        "    low_text_index = 0\n",
        "    img = keras_ocr.tools.read(img_path)\n",
        "    image = keras_ocr.detection.compute_input(img)\n",
        "    bboxes = keras_ocr.detection.getBoxes(detector.model.predict(np.array([image])), \n",
        "                                         text_threshold=0.9)\n",
        "\n",
        "    while bboxes[0].shape[0]>1:\n",
        "        bboxes = keras_ocr.detection.getBoxes(detector.model.predict(np.array([image])), \n",
        "                                             text_threshold=low_text_options[low_text_index])\n",
        "        low_text_index +=1\n",
        "    return bboxes, img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeRDe7A8ycqO"
      },
      "source": [
        "def prepare_corpus_tools(links_path, words_path): \n",
        "  ''' Function preparating the word lists and word ranges for constrained wbs'''\n",
        "  links = pd.read_csv(links_path)\n",
        "  words = pd.read_csv(words_path)\n",
        "  links = links[['0','1','2','3']]\n",
        "  links.columns = ['link_part', 'range', 'start', 'end']\n",
        "  link_dict ={}\n",
        "  for i in range(len(links)):\n",
        "    link_dict[links.iloc[i]['link_part']] = (links.iloc[i]['start'],\n",
        "                                             links.iloc[i]['end'] )\n",
        "  return link_dict, words\n",
        "\n",
        "def parse_model_arguments(model_path, character):\n",
        "  ''' Function that parses simple model arguments for the inference '''\n",
        "  parser = argparse.Namespace()\n",
        "  parser.eval_data=True\n",
        "  parser.benchmark_all_eval=False\n",
        "  parser.workers=4\n",
        "  parser.batch_size=1\n",
        "  parser.saved_model=model_path\n",
        "  parser.batch_max_length=1\n",
        "  parser.imgH=32\n",
        "  parser.imgW=100\n",
        "  parser.rgb=False\n",
        "  parser.character=character\n",
        "  parser.sensitive=False\n",
        "  parser.PAD=False\n",
        "  parser.Transformation='TPS'\n",
        "  parser.FeatureExtraction='ResNet'\n",
        "  parser.SequenceModeling='BiLSTM'\n",
        "  parser.Prediction='CTC'\n",
        "  parser.num_fiducial=20\n",
        "  parser.input_channel=1\n",
        "  parser.output_channel=512\n",
        "  parser.hidden_size=256\n",
        "  parser.num_gpu=0\n",
        "\n",
        "  opt = parser\n",
        "\n",
        "  opt.num_gpu = torch.cuda.device_count()\n",
        "  return opt\n",
        "\n",
        "def create_model(opt, device, converter):\n",
        "  opt.num_class = len(converter.character)\n",
        "\n",
        "\n",
        "  model = Model(opt)\n",
        "  model = torch.nn.DataParallel(model).to(device)\n",
        "\n",
        "  model.load_state_dict(torch.load(opt.saved_model, map_location=torch.device('cuda')))\n",
        "  model.eval()\n",
        "  print(\"model ready\")\n",
        "  return model\n",
        "\n",
        "def create_dataloader(data_path, opt):\n",
        "  AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
        "  demo_data = RawDataset(root=data_path, opt=opt)  # use RawDataset\n",
        "  demo_loader = torch.utils.data.DataLoader(\n",
        "      demo_data, batch_size=opt.batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=int(opt.workers),\n",
        "      collate_fn=AlignCollate_demo, pin_memory=True)\n",
        "  return demo_loader\n",
        "\n",
        "def decode_prediction(preds, preds_str, image_path_list, words, link_dict, \n",
        "                      corpus_full, character):\n",
        "  preds_np = preds.cpu().numpy()\n",
        "  x = np.array([np.array([np.append(x[1:], x[0])]) for x in preds_np[0]])\n",
        "  image_name = image_path_list[0].split('/')[-1]\n",
        "  corpus = create_corpus(image_name, words, link_dict)\n",
        "\n",
        "  res1 = wbs_decode(x, corpus, character, character)\n",
        "  res2 = wbs_decode(x, corpus_full, character, character)\n",
        "\n",
        "  temp_res = (f'{image_name}', res1[1], res2[1], preds_str[0], round(float(confidence_score.cpu().numpy()),4))\n",
        "\n",
        "  return temp_res\n",
        "  \n",
        "def predict(image, model, batch_size, device, opt, converter):\n",
        "  length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
        "  text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
        "  preds = model(image, text_for_pred)\n",
        "  preds_prob = F.softmax(preds, dim=2)\n",
        "  preds_max_prob, _ = preds_prob.max(dim=2)\n",
        "  confidence_score = preds_max_prob[0].cumprod(dim=0)[-1]\n",
        "  preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "  _, preds_index = preds.max(2)\n",
        "  preds_str = converter.decode(preds_index, preds_size)\n",
        "  return preds, preds_str, confidence_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sQsuCmBBOGP"
      },
      "source": [
        "\n",
        "def calculate_levenshtein_score(s: str, t: str) -> float:\n",
        "    return editdistance.eval(s, t) / max(len(s), len(t))\n",
        "\n",
        "def eval_predictions(act, pred):\n",
        "    match = [x==y for x,y in zip(act, pred)]\n",
        "    accuracy = sum(match)/len(match)\n",
        "\n",
        "    edit = [editdistance.eval(x, y) for x,y in zip(act, pred)]\n",
        "    edit_distance = sum(edit)/len(edit)\n",
        "\n",
        "    normalised_edit = [calculate_levenshtein_score(x, y) for x,y in zip(act, \n",
        "                                                                        pred)]\n",
        "    normalised_edit_distance = sum(normalised_edit)/len(normalised_edit)                                                                  \n",
        "    \n",
        "    return accuracy, normalised_edit, normalised_edit_distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNfyqsgOzeNT"
      },
      "source": [
        "# Pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZC8WS6Dzc9e"
      },
      "source": [
        "character=\"0123456789abcdefghijklmnoprstuwyzóąćęłńśźż\"\n",
        "cudnn.benchmark = True\n",
        "cudnn.deterministic = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ltJjzMjxF0v"
      },
      "source": [
        "#Preparation of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiNWFzFUpExC",
        "outputId": "682ce6d2-161d-454e-b6ba-20aa1006597f"
      },
      "source": [
        "%cd /content\n",
        "!gdown --id 1GWlf5bVi0pCM8Ixyqza6TJlMtA836eq9\n",
        "!gdown --id 1PFKI761GLTuZYFV2vGuv4MAblNJQlKzI\n",
        "\n",
        "link_dict, words = prepare_corpus_tools(links_path='/content/link_to_range100k.csv',\n",
        "                                         words_path='/content/words_in_order.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GWlf5bVi0pCM8Ixyqza6TJlMtA836eq9\n",
            "To: /content/link_to_range100k.csv\n",
            "100% 43.8k/43.8k [00:00<00:00, 33.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PFKI761GLTuZYFV2vGuv4MAblNJQlKzI\n",
            "To: /content/words_in_order.csv\n",
            "100% 1.62M/1.62M [00:00<00:00, 103MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldOlrrjMyh8M"
      },
      "source": [
        "# Prepare model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTC52fJZpNDg",
        "outputId": "039d109d-35c4-4211-d6bd-95e1d319b6f0"
      },
      "source": [
        "!gdown --id 1FocnxQzFBIjDT2F9BkNUiLdo1cC3eaO0\n",
        "!gdown --id 1o8GYwkDWJ_hE76C9z4iyh0GfBLf45jTK"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FocnxQzFBIjDT2F9BkNUiLdo1cC3eaO0\n",
            "To: /content/TPS-ResNet-BiLSTM-CTC.pth\n",
            "196MB [00:01, 125MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o8GYwkDWJ_hE76C9z4iyh0GfBLf45jTK\n",
            "To: /content/best_accuracy.pth\n",
            "196MB [00:02, 80.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnmbKhkAob3c",
        "outputId": "fb139727-f964-46cf-c374-ea92e90d9680"
      },
      "source": [
        "opt = parse_model_arguments(model_path='/content/best_accuracy.pth', \n",
        "                      character=character)\n",
        "converter = CTCLabelConverter(opt.character)\n",
        "model = create_model(opt, device, converter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJdrQ_YU2IpK",
        "outputId": "77f33d6a-5664-4486-b46a-fde03dc5ff6b"
      },
      "source": [
        "%cd /content\n",
        "!mkdir data\n",
        "!gdown --id 1Fz7MdKwV1MtL6Eh5HUNh9lAoVW1O5CqA \n",
        "!gdown --id 1IyoszVzyR9GzVQAGobcuR2EDMzZPv309\n",
        "!unzip -q new_dataset.zip -d data\n",
        "\n",
        "data = pd.read_csv('predictions_output_ver2.csv')\n",
        "data['file_name_nfd'] = [unicodedata.normalize('NFD', file_name) \\\n",
        "  for file_name in data['file_name']]\n",
        "# Sanity check to see if we can map files from the system to the results in the \n",
        "# pandas Data Frame\n",
        "# len([file_name for file_name in os.listdir('data/new_dataset') \\\n",
        "#     if file_name in data['file_name_nfd'].values])\n",
        "#data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Fz7MdKwV1MtL6Eh5HUNh9lAoVW1O5CqA\n",
            "To: /content/predictions_output_ver2.csv\n",
            "2.72MB [00:00, 87.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IyoszVzyR9GzVQAGobcuR2EDMzZPv309\n",
            "To: /content/new_dataset.zip\n",
            "288MB [00:01, 166MB/s]\n",
            "replace data/new_dataset/HistJezXVIIw_Ten10_0279.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAHP0s0_2l2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7ae493-7ff5-4817-d604-cbdf61d4f739"
      },
      "source": [
        "%cd /content\n",
        "corpus_full = create_full_corpus(words)\n",
        "resss= []\n",
        "errors_list = {}\n",
        "demo_loader = create_dataloader(data_path='data/new_dataset/', opt=opt)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for image_tensors, image_path_list in tqdm(demo_loader):\n",
        "    batch_size = image_tensors.size(0)\n",
        "    image = image_tensors.to(device)\n",
        "    try:\n",
        "\n",
        "      # predict\n",
        "      preds, preds_str, confidence_score = predict(image, model, batch_size, \n",
        "                                                   device, opt, converter)\n",
        "      # postprocess\n",
        "      temp_res = decode_prediction(preds, preds_str, image_path_list, words, link_dict, \n",
        "                                   corpus_full, character )\n",
        "      resss.append(temp_res)\n",
        "\n",
        "    except: \n",
        "      errors_list[image_path_list[0]] = 1\n",
        "    \n",
        "#resss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19999/19999 [49:01<00:00,  6.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB0SD5J7C0pD"
      },
      "source": [
        ")resss = pd.DataFrame(resss)\n",
        "resss.columns = ['file_name', 'wbs', 'wbs_full', 'model', 'conf']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ur7ZQxQChwc"
      },
      "source": [
        "!gdown --id 1ZoMRbML-4x9d5_uonQ3DuwGoBqsvuuoH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5ZweSot6qgF"
      },
      "source": [
        "data = pd.read_csv('17th_century_labels.csv')\n",
        "data['file_name_nfd'] = [unicodedata.normalize('NFD', file_name) \\\n",
        "  for file_name in data['file_name']]\n",
        "data.merge(resss, on='file_name', how = 'left')\n",
        "eval_predictions(data.label, data.wbs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX65UD35DgCQ"
      },
      "source": [
        "eval_predictions(data.label, data.wbs_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijs-zy8SDiQZ"
      },
      "source": [
        "eval_predictions(data.label, data.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouwxnfOnDjhE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
